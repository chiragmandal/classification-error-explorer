{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spotify-Annoy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5915U3yqSS_",
        "outputId": "cad52a36-004b-4637-b0ff-455e21e04c3c"
      },
      "source": [
        "!pip install annoy"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting annoy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz (646kB)\n",
            "\r\u001b[K     |▌                               | 10kB 17.4MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 18.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 10.2MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 4.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71kB 5.3MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 143kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 194kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 215kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 235kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 266kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 286kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 307kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 317kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 337kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 358kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 378kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 389kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 399kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 409kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 430kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 440kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 450kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 460kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 471kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 481kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 501kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 512kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 522kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 532kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 542kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 552kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 563kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 573kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 593kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 614kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 634kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 655kB 5.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp36-cp36m-linux_x86_64.whl size=390355 sha256=816131517405ca9c95298d4182957727122f5c9a0fa53e4909f3b8a97017b4c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/c5/59/cce7e67b52c8e987389e53f917b6bb2a9d904a03246fadcb1e\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx3qVuCUVkEn"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        "\r\n",
        "import glob        #for reading file names in a folder\r\n",
        "import os.path        \r\n",
        "from google.colab import drive\r\n",
        "from annoy import AnnoyIndex"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdNiQ4CZqYXP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN4-MYOsIMLH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f86be10f-3cc2-4fd9-c756-74e5151bd60c"
      },
      "source": [
        "drive.mount('/content/drive')\r\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/input/') #Add your path of images here"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WTKYVpQOioD3",
        "outputId": "aa3294b7-cfdd-4a2a-f2f1-4ab220ca5db7"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/input'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyuvsipenNfP",
        "outputId": "c37568d4-820e-4323-b484-9d71bf54d99c"
      },
      "source": [
        "path = os.path.expanduser('/content/drive/My Drive/Colab Notebooks/input/test/')\r\n",
        "print(path)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/input/test/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FmqCazOYwgK"
      },
      "source": [
        "#function to take input image\r\n",
        "def input_img(path):\r\n",
        "  \r\n",
        "  image = tf.io.read_file(path)  #reads the input image file and returns the datatype of string\r\n",
        "  image = tf.io.decode_jpeg(image, channels=3)      # Decodes the image to W x H x 3 shape tensor with type of uint8\r\n",
        "  image = tf.image.resize_with_pad(image, 224, 224)    # Resize the image to 224 x 244 x 3 shape tensor\r\n",
        "\r\n",
        "  image  = tf.image.convert_image_dtype(image, tf.float32)[tf.newaxis, ...]\r\n",
        "\r\n",
        "  return image\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qatS2yqvQpid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b22fe988-72d0-4f63-9ab4-6e0425af33f7"
      },
      "source": [
        "def get_image_feature_vectors():\r\n",
        "\r\n",
        "  i=0\r\n",
        "  start_time = time.time()\r\n",
        "    \r\n",
        "  print(\"---------------------------------\")\r\n",
        "  print(\"Step.1 of 2 - mobilenet_v2_140_224 - Loading Started at %s\" %time.ctime())\r\n",
        "  print(\"---------------------------------\")\r\n",
        "\r\n",
        "  # Definition of module with using tfhub.dev handle\r\n",
        "  module_handle = \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\" \r\n",
        "  \r\n",
        "  # Load the module\r\n",
        "  module = hub.load(module_handle)\r\n",
        "\r\n",
        "  print(\"---------------------------------\")\r\n",
        "  print(\"Step.1 of 2 - mobilenet_v2_140_224 - Loading Completed at %s\" %time.ctime())\r\n",
        "  print(\"--- %.2f minutes passed ---------\" % ((time.time() - start_time)/60))\r\n",
        "\r\n",
        "  print(\"---------------------------------\")\r\n",
        "  print(\"Step.2 of 2 - Generating Feature Vectors -  Started at %s\" %time.ctime())\r\n",
        "  #for filename in glob.glob('/Users/erdemisbilen/Angular/fashionWebScraping/images_scraped/full/test/*.jpg'): #assuming gif\r\n",
        "  for filename in glob.glob('*.jpg'): #assuming gif\r\n",
        "    i = i + 1\r\n",
        "\r\n",
        "    print(\"-----------------------------------------------------------------------------------------\")\r\n",
        "    print(\"Image count                     :%s\" %i)\r\n",
        "    print(\"Image in process is             :%s\" %filename)\r\n",
        "\r\n",
        "    # Loads and pre-process the image\r\n",
        "    img = input_img(filename)\r\n",
        "\r\n",
        "    # Calculate the image feature vector of the img\r\n",
        "    features = module(img)   \r\n",
        "  \r\n",
        "    # Remove single-dimensional entries from the 'features' array\r\n",
        "    feature_set = np.squeeze(features)  \r\n",
        "\r\n",
        "    # Saves the image feature vectors into a file for later use\r\n",
        "\r\n",
        "    outfile_name = os.path.basename(filename).split('.')[0] + \".npz\"\r\n",
        "    #out_path = os.path.join('/Users/erdemisbilen/Angular/fashionWebScraping/images_scraped/feature-vectors/test/', outfile_name)\r\n",
        "    out_path = os.path.join(path, outfile_name)\r\n",
        "    # Saves the 'feature_set' to a text file\r\n",
        "    np.savetxt(out_path, feature_set, delimiter=',')\r\n",
        "\r\n",
        "    print(\"Image feature vector saved to   :%s\" %out_path)\r\n",
        "  \r\n",
        "  print(\"---------------------------------\")\r\n",
        "  print (\"Step.2 of 2 - Generating Feature Vectors - Completed at %s\" %time.ctime())\r\n",
        "  print(\"--- %.2f minutes passed ---------\" % ((time.time() - start_time)/60))\r\n",
        "  print(\"--- %s images processed ---------\" %i)\r\n",
        "    \r\n",
        "get_image_feature_vectors()\r\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Step.1 of 2 - mobilenet_v2_140_224 - Loading Started at Sun Dec 13 19:44:36 2020\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "Step.1 of 2 - mobilenet_v2_140_224 - Loading Completed at Sun Dec 13 19:45:41 2020\n",
            "--- 1.07 minutes passed ---------\n",
            "---------------------------------\n",
            "Step.2 of 2 - Generating Feature Vectors -  Started at Sun Dec 13 19:45:41 2020\n",
            "-----------------------------------------------------------------------------------------\n",
            "Image count                     :1\n",
            "Image in process is             :dd_tree.jpg\n",
            "Image feature vector saved to   :/content/drive/My Drive/Colab Notebooks/input/test/dd_tree.npz\n",
            "-----------------------------------------------------------------------------------------\n",
            "Image count                     :2\n",
            "Image in process is             :snake.jpg\n",
            "Image feature vector saved to   :/content/drive/My Drive/Colab Notebooks/input/test/snake.npz\n",
            "---------------------------------\n",
            "Step.2 of 2 - Generating Feature Vectors - Completed at Sun Dec 13 19:45:41 2020\n",
            "--- 1.08 minutes passed ---------\n",
            "--- 2 images processed ---------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNIecbq7w_3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ece927-fe23-4774-b257-fd25dd0c629f"
      },
      "source": [
        "# Numpy for loading image feature vectors from file\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Time for measuring the process time\r\n",
        "import time\r\n",
        "\r\n",
        "# Glob for reading file names in a folder\r\n",
        "import glob\r\n",
        "import os.path\r\n",
        "\r\n",
        "# json for storing data in json file\r\n",
        "import json\r\n",
        "\r\n",
        "# Annoy and Scipy for similarity calculation\r\n",
        "from annoy import AnnoyIndex\r\n",
        "from scipy import spatial\r\n",
        "#################################################\r\n",
        "\r\n",
        "#################################################\r\n",
        "# This function reads from 'image_data.json' file\r\n",
        "# Looks for a specific 'filename' value\r\n",
        "# Returns the product id when product image names are matched \r\n",
        "# So it is used to find product id based on the product image name\r\n",
        "#################################################\r\n",
        "def match_id(filename):\r\n",
        "  #with open('/Users/erdemisbilen/Angular/fashionWebScraping/jsonFiles/image_data.json') as json_file:\r\n",
        "  with open('/image_data.json') as json_file:\r\n",
        "    \r\n",
        "    for file in json_file:\r\n",
        "        seen = json.loads(file)\r\n",
        "\r\n",
        "        for line in seen:\r\n",
        "          \r\n",
        "          if filename==line['imageName']:\r\n",
        "            print(line)\r\n",
        "            return line['productId']\r\n",
        "            break\r\n",
        "#################################################\r\n",
        "\r\n",
        "#################################################\r\n",
        "# This function; \r\n",
        "# Reads all image feature vectores stored in /feature-vectors/*.npz\r\n",
        "# Adds them all in Annoy Index\r\n",
        "# Builds ANNOY index\r\n",
        "# Calculates the nearest neighbors and image similarity metrics\r\n",
        "# Stores image similarity scores with productID in a json file\r\n",
        "#################################################\r\n",
        "def cluster():\r\n",
        "\r\n",
        "  start_time = time.time()\r\n",
        "  \r\n",
        "  print(\"---------------------------------\")\r\n",
        "  print (\"Step.1 - ANNOY index generation - Started at %s\" %time.ctime())\r\n",
        "  print(\"---------------------------------\")\r\n",
        "\r\n",
        "  # Defining data structures as empty dict\r\n",
        "  file_index_to_file_name = {}\r\n",
        "  file_index_to_file_vector = {}\r\n",
        "  file_index_to_product_id = {}\r\n",
        "\r\n",
        "  # Configuring annoy parameters\r\n",
        "  dims = 1792\r\n",
        "  n_nearest_neighbors = 20\r\n",
        "  trees = 10000\r\n",
        "  \r\n",
        "  tmp_path = path + '/test/*.npz'\r\n",
        "  # Reads all file names which stores feature vectors \r\n",
        "  allfiles = glob.glob(tmp_path)\r\n",
        "  \r\n",
        "  t = AnnoyIndex(dims, metric='angular')\r\n",
        "\r\n",
        "  for file_index, i in enumerate(allfiles):\r\n",
        "    \r\n",
        "  \r\n",
        "    # Reads feature vectors and assigns them into the file_vector \r\n",
        "    file_vector = np.loadtxt(i)\r\n",
        "\r\n",
        "    # Assigns file_name, feature_vectors and corresponding product_id\r\n",
        "    file_name = os.path.basename(i).split('.')[0]\r\n",
        "    file_index_to_file_name[file_index] = file_name\r\n",
        "    file_index_to_file_vector[file_index] = file_vector\r\n",
        "    file_index_to_product_id[file_index] = match_id(file_name)\r\n",
        "\r\n",
        "    # Adds image feature vectors into annoy index   \r\n",
        "    t.add_item(file_index, file_vector)\r\n",
        "\r\n",
        "    print(\"---------------------------------\")\r\n",
        "    print(\"Annoy index     : %s\" %file_index)\r\n",
        "    print(\"Image file name : %s\" %file_name)\r\n",
        "    print(\"Product id      : %s\" %file_index_to_product_id[file_index])\r\n",
        "    print(\"--- %.2f minutes passed ---------\" % ((time.time() - start_time)/60))\r\n",
        "\r\n",
        "\r\n",
        "  # Builds annoy index\r\n",
        "  t.build(trees)\r\n",
        "\r\n",
        "  print (\"Step.1 - ANNOY index generation - Finished\")\r\n",
        "  print (\"Step.2 - Similarity score calculation - Started \") \r\n",
        "  \r\n",
        "  named_nearest_neighbors = []\r\n",
        "\r\n",
        "  # Loops through all indexed items\r\n",
        "  for i in file_index_to_file_name.keys():\r\n",
        "\r\n",
        "    # Assigns master file_name, image feature vectors and product id values\r\n",
        "    master_file_name = file_index_to_file_name[i]\r\n",
        "    master_vector = file_index_to_file_vector[i]\r\n",
        "    master_product_id = file_index_to_product_id[i]\r\n",
        "\r\n",
        "    # Calculates the nearest neighbors of the master item\r\n",
        "    nearest_neighbors = t.get_nns_by_item(i, n_nearest_neighbors)\r\n",
        "\r\n",
        "    # Loops through the nearest neighbors of the master item\r\n",
        "    for j in nearest_neighbors:\r\n",
        "\r\n",
        "      print(j)\r\n",
        "\r\n",
        "      # Assigns file_name, image feature vectors and product id values of the similar item\r\n",
        "      neighbor_file_name = file_index_to_file_name[j]\r\n",
        "      neighbor_file_vector = file_index_to_file_vector[j]\r\n",
        "      neighbor_product_id = file_index_to_product_id[j]\r\n",
        "\r\n",
        "      # Calculates the similarity score of the similar item\r\n",
        "      similarity = 1 - spatial.distance.cosine(master_vector, neighbor_file_vector)\r\n",
        "      rounded_similarity = int((similarity * 10000)) / 10000.0\r\n",
        "\r\n",
        "      # Appends master product id with the similarity score \r\n",
        "      # and the product id of the similar items\r\n",
        "      named_nearest_neighbors.append({\r\n",
        "        'similarity': rounded_similarity,\r\n",
        "        'master_pi': master_product_id,\r\n",
        "        'similar_pi': neighbor_product_id})\r\n",
        "\r\n",
        "    print(\"---------------------------------\") \r\n",
        "    print(\"Similarity index       : %s\" %i)\r\n",
        "    print(\"Master Image file name : %s\" %file_index_to_file_name[i]) \r\n",
        "    print(\"Nearest Neighbors.     : %s\" %nearest_neighbors) \r\n",
        "    print(\"--- %.2f minutes passed ---------\" % ((time.time() - start_time)/60))\r\n",
        "\r\n",
        "  \r\n",
        "  print (\"Step.2 - Similarity score calculation - Finished \") \r\n",
        "\r\n",
        "  # Writes the 'named_nearest_neighbors' to a json file\r\n",
        "  with open('nearest_neighbors.json', 'w') as out:\r\n",
        "    json.dump(named_nearest_neighbors, out)\r\n",
        "\r\n",
        "  print (\"Step.3 - Data stored in 'nearest_neighbors.json' file \") \r\n",
        "  print(\"--- Prosess completed in %.2f minutes ---------\" % ((time.time() - start_time)/60))\r\n",
        "\r\n",
        "cluster()\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Step.1 - ANNOY index generation - Started at Sun Dec 13 19:53:01 2020\n",
            "---------------------------------\n",
            "Step.1 - ANNOY index generation - Finished\n",
            "Step.2 - Similarity score calculation - Started \n",
            "Step.2 - Similarity score calculation - Finished \n",
            "Step.3 - Data stored in 'nearest_neighbors.json' file \n",
            "--- Prosess completed in 0.01 minutes ---------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}